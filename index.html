<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces">
  <meta name="keywords" content="Functional Scene Graph, 3D Scene Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenFunGraph ðŸ›‹</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">OpenFunGraph<br />Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces<br />
            <!-- <p class="title is-3 publication-title">Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces </p> -->
          </h1>          

          <h1 class="title is-4" style="color: #5c5c5c;">CVPR 2025 (Highlight)</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhangcyg.github.io/">Chenyangguang Zhang</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://alexdelitzas.github.io/">Alexandros Delitzas</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://fangjinhuawang.github.io/">Fangjinhua Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://lolrudy.github.io/">Ruida Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a><sup>2,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://francisengelmann.github.io/">Francis Engelmann</a><sup>2,5</sup>
            </span>
            <span class="author-block">
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 1em;"><sup>1</sup>Tsinghua University</span>
            <span class="author-block" style="margin-right: 1em;"><sup>2</sup>ETH ZÃ¼rich</span>
            <span class="author-block" style="margin-right: 1em;"><sup>3</sup>MPI for Informatics</span>
            <span class="author-block" style="margin-right: 1em;"><sup>4</sup>Microsoft</span>
            <span class="author-block" style="margin-right: 1em;"><sup>5</sup>Stanford University</span></br>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.19199"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/ZhangCYG/OpenFunGraph"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/OpenFunGraph"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-solid"></i>ðŸ¤—
                  </span>
                  <span>Datasets</span>
                  </a>
              </span>
    
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" align="center">
      <img src="static/images/teaser_top.jpg" class="center" width="50%" />
      <img src="static/images/teaser.png" class="center" width="50%" />
      <br><br>
      <h2 class="subtitle has-text-left">
      <strong>Functional 3D Scene Graphs.</strong> 
      Given an input sequence of posed RGB-D frames of an indoor environment, our method predicts a functional 3D scene graph by detecting objects, identifying interactive elements, and inferring functional relationships. 
      This enables the representation of interactions, functions, and scene dynamics, going beyond existing 3D scene graph methods that are constrained to spatial relationships between static objects.
    </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce the task of predicting functional 3D scene graphs for real-world indoor environments from posed RGB-D images. 
            Unlike traditional 3D scene graphs that focus on spatial relationships of objects, functional 3D scene graphs capture objects, interactive elements, and their functional relationships. 
            Due to the lack of training data, we leverage foundation models, including visual language models (VLMs) and large language models (LLMs), to encode functional knowledge. 
            We evaluate our approach on an extended SceneFun3D dataset and a newly collected dataset, FunGraph3D, both annotated with functional 3D scene graphs. 
            Our method significantly outperforms adapted baselines, including Open3DSG and ConceptGraph, demonstrating its effectiveness in modeling complex scene functionalities. 
            We also demonstrate downstream applications such as 3D question answering and robotic manipulation using functional 3D scene graphs.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
       
        <h2 class="title is-3">Qualitative results</h2>
        <img src="static/images/quali.jpg" class="center"/>
      <br>
        <div class="content has-text-justified">
          <p>
            Scene interactive elements and their functional relationships between objects are exploited by our approach.
          </p>
        </div>
        <video controls poster="static/images/quali_res_new_output_poster.png">
          <source src="static/videos/quali_res_new_output.mp4" type="video/mp4">
          </video>
          <!-- <br>
          <video controls poster="static/images/quali_res_poster2.PNG">
            <source src="static/videos/quali_res2.mp4" type="video/mp4">
            </video> -->
          <br><br>
          <h2 class="title is-3">FunGraph3D Dataset</h2>
          <img src="static/images/masks.jpg" class="center"/>
          <img src="static/images/graph.jpg" class="center"/>
          <img src="static/images/hands.jpg" class="center"/>
          <img src="static/images/scenes.jpg" class="center"/>
        <br>
          <div class="content has-text-justified">
            <p>
              FunGraph3D dataset is constructed with multi-sensor data (i.e., high-fidelity 3D reconstructions, consumer-device video captures, egocentric human-scene interaction videos) and functional 3D scene graph annotations. 
            </p>
          </div>
        <h2 class="title is-4">High-resolution 3D scan</h2>
        <video controls poster="static/images/video3d1.png">
          <source src="static/videos/scan_3d.mp4" type="video/mp4">
          </video>
          <br><br>
          <h2 class="title is-4">Static RGB-D sequence</h2>
          <video controls poster="static/images/videorgb1.png">
            <source src="static/videos/rgb.mp4" type="video/mp4">
            </video>
            <br><br>
        <h2 class="title is-4">Egocentric human-scene interaction video</h2>
        <video controls poster="static/images/videoego1.png">
          <source src="static/videos/ego.MP4" type="video/mp4">
          </video>
          <br><br>
        <h2 class="title is-4">Functional 3D scene graph annotations</h2>
        <video controls poster="static/images/videoanno1.png">
          <source src="static/videos/anno.mp4" type="video/mp4">
          </video>
      </div>
    </div>
    <h2 class="title is-3">BibTeX</h2>
    <pre><code>@inproceedings{zhang2025open,
  title = {{Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces}},
  author = {Zhang, Chenyangguang and Delitzas, Alexandros and Wang, Fangjinhua and Zhang, Ruida and Ji, Xiangyang and Pollefeys, Marc and Engelmann, Francis},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2025}
}</code></pre>
    </section>
  </div>
</body>
</html>


